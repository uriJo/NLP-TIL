**지도학습과 비지도학습**

지도학습 : 정답(라벨)이 있는 data를 활용해 학습시키는 방법

1) classification(Knn, naive bayes, support vector, machine decision)

2) regression(linear regression, lasso)

비지도학습 : 정답이 없는 데이터를 비슷한 특징끼리 군집화 하여 나타난 결과를 예측하는 방법

1) clustering 2)K means 3) GAN(generative adversarial network)

강화학습 : 행동 심리학에서 나온 보상을 받으며 학습하는 방법 (아직은 잘 모르겠음) 


**train data, test data**

train data : 모델을 학습할때 사용하는 데이터

test data : 학습한 모델을 평가할때 사용하는 데이터

8:2, 7:3의 비중을 두고 분리한다.

모델 데이터를 학습하기 전, **전처리 하는 과정**이 매우 중요하다.

ex) 스케일이 다른 경우 원치 않는 결과 값을 가져 올 수 있다. → 정규화 

 누락 데이터 → Na/NaN, 0, 평균값, 중간값 으로 채우기 등

 텍스트 → 숫자로 변환
 

**K 근접 이웃  회귀** 

- 사이킷런에서 사용할 훈련 데이터는 2차원 배열 형태 → reshape() 이용

ex> [1, 2, 3]의 키 데이터가 있을 때 → [[1], [2], [3]] 

(3,) → (3,1) 2차원 배열의 형태로 바꿔줘야 한다.


**overfitting과 underfitting**

훈련 데이터에서 좋았는데 테스트 데이터에서 점수가 나쁠 경우 과대 적합 되었다고 말한다. → overfitting

<img width="435" alt="스크린샷 2022-01-20 오후 11 29 19" src="https://user-images.githubusercontent.com/98019312/150487088-1bdcd4b8-5825-460d-ba98-10199ef3b08c.png">


반대로 훈련 데이터보다 테스트 데이터의 점수가 높은 경우, 또는 둘다 너무 낮은 경우는 과소 적합되었다고 한다. → underfitting


**overfitting과 underfitting 탐지 방법**

<img width="474" alt="스크린샷 2022-01-20 오후 11 31 41" src="https://user-images.githubusercontent.com/98019312/150486962-0ce93f59-54dc-487a-a0b1-2ee18eedc73d.png">


모델의 복잡도와 손실함수에 따른 발생 구간을 분류할 수 있음

**과대 적합 발생 구간**

모델이 점차 복잡해질수록 gradient descent 기법을 중심으로 모델이 학습되어 train loss는 감소하는 동시에 test loss가 증가하게 된다.


**손실함수**

모델이 예측한 답과 원래 정답의 오차를 표현, 판단하는 함수를 말한다. 

성능을 증명하는 중요한 지표 중 하나다.

**MSE(Mean Squared Error, 평균 제곱 오차)** : 모델이 예측한 값과 실제 정답 값의 차를 제곱하여 모두 더한 후 평균을 낸다. 제곱하는 이유는 절대값을 사용하기 위해서다. 

**RMSE(Root Squared Error)** : MSE 방식에 루트를 씌운 것. 값의 왜곡을 보정한다.

**Binary Crossentropy** : 이진 분류시 사용하는 손실 함수

**Categorical Crossentropy** : 다중 분류시 사용하는 손실 함수


선형 회귀 모델에 규제를 추가한 모델 왜 ? 너무 overfit 되지 않도록 하기 위해서

**릿지(ridge), 라쏘(lasso)**

**릿지 회귀**

모델 객체를 만들때 alpha를 매개변수로 규제의 강도를 조절한다. alpha값이 크면 규제 강도가 커지므로 계수값을 줄이고 과소적합 되도록 유도한다. 반대로 alpha값이 작아지면 계수 줄이는 역할이 줄어들고 선형 회귀 모델과 비슷해진다. 

이 alpha값은 모델이 학습하는  값이 아니라 사전에 사람이 정해 주어야 한다. 머신러닝 모델이 학습할 수 없고 사람이 알려줘야 하는 파라미터를 하이퍼 파라미터(hyperparameter)라고 한다
<img width="533" alt="스크린샷 2022-01-21 오후 3 02 37" src="https://user-images.githubusercontent.com/98019312/150487234-ed1af3ae-cf00-4172-89fc-2bee70488d40.png">



**라쏘 회귀** 

<img width="460" alt="스크린샷 2022-01-21 오후 3 02 57" src="https://user-images.githubusercontent.com/98019312/150487271-f151ee1a-18a9-40e1-ae94-f7e6fb2f7310.png">

패털티 항(alpha)에 절대값을 주었다.

라쏘 회귀 경우 몇몇 유의미하지 않은 변수들에 대한 계수를 0에 가깝게 추정해 변수 선택 효과를 가져온다. 

활성화 함수

**시그모이드 함수(sigmoid)**

로지스틱 회귀 분석 또는 이진 분류의 마지막 레이어의 활성 함수로 사용된다.

<img width="121" alt="스크린샷 2022-01-21 오후 3 12 02" src="https://user-images.githubusercontent.com/98019312/150487375-cfbcd338-da14-48fa-a437-01bfd4eaaef2.png">
<img width="403" alt="스크린샷 2022-01-21 오후 3 13 24" src="https://user-images.githubusercontent.com/98019312/150487380-f5ef7ef8-6738-461c-83d3-b047ebb52671.png">


0 ~ 1 사이의 범위를 가져 확률로서의 의미를 가질 수 있다. 

**소프트맥스 함수 (softmax)**

세개 이상으로 분류하는 다중 클래스 분류에서 사용되는 활성화함수다.

분류될 클래스가 n개라고 할때 n차원의 벡터를 입력받아 각 클래스에 속할 확률을 추정한다.

만약 분류될 클래스가 3개라고 한다면 다음과 같이 계산된다.  (e 상수)
<img width="459" alt="스크린샷 2022-01-21 오후 3 22 42" src="https://user-images.githubusercontent.com/98019312/150487318-8a4866ea-5a1c-4815-884e-0f1e489d767c.png">
<img width="492" alt="스크린샷 2022-01-21 오후 3 25 12" src="https://user-images.githubusercontent.com/98019312/150487393-aaa0c59b-ed60-463c-a100-3009fa2de4eb.png">


**확률적 경사 하강법(Stochastic Gradient Descent)**

- 손실함수를 최소화 하기 위해선 1) optimizer(최적화) 2) 경사 하강법 방법을 사용한다.
<img width="373" alt="스크린샷 2022-01-21 오후 3 32 34" src="https://user-images.githubusercontent.com/98019312/150487480-fb82a3bc-6050-4000-9c1a-cb9f1453e0fc.png">


하지만 경사 하강법에는  한계가 존재한다. 

경사 하강법은 신경망에서 출력되는(딥러닝) 예측값과 실제 값 차이인 손실함수(loss funciton)의 값을 최소화 하는 것이 목적이다. 그러나 학습 데이터 셋이 많아질 수록 계산량은 많아지고 이로 인한 학습  속도가 느려지게 된다.
<img width="461" alt="스크린샷 2022-01-21 오후 3 36 48" src="https://user-images.githubusercontent.com/98019312/150487523-45193b00-76aa-4c1f-9bd4-c2dea43286ce.png">

또한 실제 위처럼 매끈한 2차 곡선 형태가 나오지 않는다. 이로 인해 이동거리만큼 이동 하더라고 최적  해에 수립하지 못할 수도 있다. (local minimum 현상)

또한 plateau라는 곳에서의 학습 속도가 매우 느리다. 이동 거리 = 현 지점의 기울기 X 학습률인데 기울기가 0에 가까워 질 수록 이동거리가 줄어 들어 더이상 학습이 일어나지 않는 가중치 소멸(gradient vanishing) 현상이 발생할 수 도 있다. 

또한 가중치(w)가 여러개 있을 경우 각각의 가중치마다 스케일이 다르면 다음과 같은 현상이 나타난다.

<img width="542" alt="스크린샷 2022-01-21 오후 3 39 54" src="https://user-images.githubusercontent.com/98019312/150487445-18c213dd-16fb-4dac-8a49-7092655aa26f.png">


w1의 스케일이 w2보다 크다보니 손실함수는 w1의 변화에 매우 둔감하고 w2에 매우 민감하다. 따라서 w2가 조금만 변해도 손실함수는 크게 변화게 되어 두 매개변수에 따른 손실 함수 변화가 일정하지 않다. 실제 매개 변수를 수백만개에 달할 정도로 많을테니 이러한 지그재그식 모습은 더욱 복잡해지고 이로 인해 최적의 해를 보장받기 쉽지 않다.

참고 : [https://gooopy.tistory.com/67?category=824281](https://gooopy.tistory.com/67?category=824281)



**모델 학습의 단위 - epoch, batch, size, iteration**

**epoch**

훈련 데이터 셋들이 모델을 통과한 횟수

1epoch는 훈련 데이터셋 전체가 한 신경망에 적용되어 순전파와 역전파를 통해 신경망을 한번 통과했다는 의미.

epoch를 높일 수록 적합한 파라미터 찾을 확률이 올라간다. ( overfitting 주의)

**batch size**

연산 한번에 들어가는 데이터의 크기, 그 데이터 크기만큼의 데이터셋 → mini batch

1회 epoch에 m개 이상의 mini batch가 들어가게 되며 m = 1인 경우 배치 학습법이라고 한다.

bacth size가 너무 큰 경우 한번에 처리해야 할 데이터의 양이 많아지므로 학습 속도가 느려지게 되고 메모리 부족 문제가 나타날 수 있다.

batch size가 너무 작은 경우 적은 양의 데이터로 가중치를 업데이트하고 업데이트가 자주 발생하므로 좋지 x

**iteration**

각 배치마다 파라미터 업데이트가 한번씩 일어나므로 iteration은 파라미터 업데이트 횟수와 같다.

1. 배치 경사 하강법(Batch Gradient Desent, BGD)
    
    손실함수 기울기 계신에서 전체 학습 데이터를 대상으로 동일하게 잡는 방법
    
2. 확률적 경사 하강법(Stochastic Gradient Desent, SGD)
    
    전체 훈련 데이터 셋으로 학습하는 것은 매우 비효율적 → 학습 데이터 셋에서 무작위로 한개의 샘플 데이터를 꺼내 그 데이터에 대해서만 기울기를 계산하는 것
    
    학습 속도가 매우 빠르다. ↔ 안정적이지 못하다. → 미니 배치 방법이 등장
    
3. 미니 배치 하강법(mini batch Gradient Desent)
    
    여러개의 데이터를 꺼내 수행.
    
    배치의 크기는 총학습 데이터 셋의 크기를 배치 크기로 나누었을때 떨어지는 크기로 하는 것이 좋다. 
    

**교차 검증(cross validation)**

고정된 test set을 통해 모델의 성능을 검증하고 수정하는 과정을 반복하면 내가 만든 모델은 test set에만 잘 동작된다는 문제점을 가진다. 이를 해결하고자 하는 것이 바로 교차 검증이다.

1. K-Fold Cross Validation (5 or 10을 많이 사용) → 회귀 모델일 경우
2. Stratifield K-Fold Cross Validation → 분류 모델일 경우

**Grid-Search**

모델에 가장 적합한 하이퍼파라미터 찾기 sklearn의 GridSearchCV 이용

```python
import pandas as pd

wine = pd.read_csv("https://bit.ly/wine_csv_data")

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

#sub 추출을 위해 2번 적용
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size = 0.2, random_state = 42)
sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size = 0.2, random_state = 42)

#grid-search - for best hyperparameter
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
params = {'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}
#n_jobs 병렬 실행에 사용할 CPU 수 -1 -> 시스템 내 모든 코어 사용
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs= -1)
gs.fit(train_input, train_target)
dt = gs.best_estimator_
print(dt.score(train_input, train_target))

#0.9615162593804117
```
