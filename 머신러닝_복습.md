**지도학습과 비지도학습**

지도학습 : 정답(라벨)이 있는 data를 활용해 학습시키는 방법

1) classification(Knn, naive bayes, support vector, machine decision)

2) regression(linear regression, lasso)

비지도학습 : 정답이 없는 데이터를 비슷한 특징끼리 군집화 하여 나타난 결과를 예측하는 방법

1) clustering 2)K means 3) GAN(generative adversarial network)

강화학습 : 행동 심리학에서 나온 보상을 받으며 학습하는 방법 (아직은 잘 모르겠음) 


**train data, test data**

train data : 모델을 학습할때 사용하는 데이터

test data : 학습한 모델을 평가할때 사용하는 데이터

8:2, 7:3의 비중을 두고 분리한다.

모델 데이터를 학습하기 전, **전처리 하는 과정**이 매우 중요하다.

ex) 스케일이 다른 경우 원치 않는 결과 값을 가져 올 수 있다. → 정규화 

 누락 데이터 → Na/NaN, 0, 평균값, 중간값 으로 채우기 등

 텍스트 → 숫자로 변환
 

**K 근접 이웃  회귀** 

- 사이킷런에서 사용할 훈련 데이터는 2차원 배열 형태 → reshape() 이용

ex> [1, 2, 3]의 키 데이터가 있을 때 → [[1], [2], [3]] 

(3,) → (3,1) 2차원 배열의 형태로 바꿔줘야 한다.


**overfitting과 underfitting**

훈련 데이터에서 좋았는데 테스트 데이터에서 점수가 나쁠 경우 과대 적합 되었다고 말한다. → overfitting

<img width="435" alt="스크린샷 2022-01-20 오후 11 29 19" src="https://user-images.githubusercontent.com/98019312/150487088-1bdcd4b8-5825-460d-ba98-10199ef3b08c.png">


반대로 훈련 데이터보다 테스트 데이터의 점수가 높은 경우, 또는 둘다 너무 낮은 경우는 과소 적합되었다고 한다. → underfitting


**overfitting과 underfitting 탐지 방법**

<img width="474" alt="스크린샷 2022-01-20 오후 11 31 41" src="https://user-images.githubusercontent.com/98019312/150486962-0ce93f59-54dc-487a-a0b1-2ee18eedc73d.png">


모델의 복잡도와 손실함수에 따른 발생 구간을 분류할 수 있음
1. **하이퍼 파라미터 수정**

모델이 점차 복잡해질수록 gradient descent 기법을 중심으로 모델이 학습되어 train loss는 감소하는 동시에 test loss가 증가하게 된다. 이 구간을 보고 epoch, optimizer 등 하이퍼 파라미터를 수정한다.

**콜백**

epoch가 과대 적합 되는 것을 방지하는 만큼 값을 결정하기 위해 미리 종료하는 것

```python
model = model_fn(keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', 
              metrics='accuracy')

checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5', 
                                                save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=2,
                                                  restore_best_weights=True
#restore_best_weights=True 모델의 가중치와 절편만 저장, False = 모델 전체 저장

history = model.fit(train_scaled, train_target, epochs=20, verbose=0, 
                    validation_data=(val_scaled, val_target),
                    callbacks=[checkpoint_cb, early_stopping_cb])
```

1. **드롭 아웃**
    
    <img width="451" alt="스크린샷 2022-01-23 오후 6 03 45" src="https://user-images.githubusercontent.com/98019312/150672016-51e49a00-c961-4be0-8da7-f4754c19e688.png">

    

y를 계산할때 h2, h5는 사용되지 않게 하고 역전파를 수행할때 이들에 대한 그래디언트들도 적용되지 않게 한다. 

왜 드롭아웃이 과대 적합을  막을까? 

이전 층의 일부 뉴런이 랜덤하게 꺼내지면 특정 뉴런에 과대 의존하는 것을 줄일 수 있고 일부 뉴런에 대한 출력이 없을 수 있다는 것을 감안하면 더 안정적인 예측을 만들어 내는데 주의를 기울이기 때문이다.

훈련된 모든 뉴런을 사용해야 예측이 올바르게 수행할수 있으므로 훈련이 끝난뒤 평가나 예측 수행시 드롭 아웃을 적용하지 말아야 한다.


**손실함수**

모델이 예측한 답과 원래 정답의 오차를 표현, 판단하는 함수를 말한다. 

성능을 증명하는 중요한 지표 중 하나다.

**MSE(Mean Squared Error, 평균 제곱 오차)** : 모델이 예측한 값과 실제 정답 값의 차를 제곱하여 모두 더한 후 평균을 낸다. 제곱하는 이유는 절대값을 사용하기 위해서다. 

**RMSE(Root Squared Error)** : MSE 방식에 루트를 씌운 것. 값의 왜곡을 보정한다.

**Binary Crossentropy** : 이진 분류시 사용하는 손실 함수

**Categorical Crossentropy** : 다중 분류시 사용하는 손실 함수


선형 회귀 모델에 규제를 추가한 모델 왜 ? 너무 overfit 되지 않도록 하기 위해서

**릿지(ridge), 라쏘(lasso)**

**릿지 회귀**

모델 객체를 만들때 alpha를 매개변수로 규제의 강도를 조절한다. alpha값이 크면 규제 강도가 커지므로 계수값을 줄이고 과소적합 되도록 유도한다. 반대로 alpha값이 작아지면 계수 줄이는 역할이 줄어들고 선형 회귀 모델과 비슷해진다. 

이 alpha값은 모델이 학습하는  값이 아니라 사전에 사람이 정해 주어야 한다. 머신러닝 모델이 학습할 수 없고 사람이 알려줘야 하는 파라미터를 하이퍼 파라미터(hyperparameter)라고 한다
<img width="533" alt="스크린샷 2022-01-21 오후 3 02 37" src="https://user-images.githubusercontent.com/98019312/150487234-ed1af3ae-cf00-4172-89fc-2bee70488d40.png">



**라쏘 회귀** 

<img width="460" alt="스크린샷 2022-01-21 오후 3 02 57" src="https://user-images.githubusercontent.com/98019312/150487271-f151ee1a-18a9-40e1-ae94-f7e6fb2f7310.png">

패털티 항(alpha)에 절대값을 주었다.

라쏘 회귀 경우 몇몇 유의미하지 않은 변수들에 대한 계수를 0에 가깝게 추정해 변수 선택 효과를 가져온다. 

활성화 함수

**시그모이드 함수(sigmoid)**

로지스틱 회귀 분석 또는 이진 분류의 마지막 레이어의 활성 함수로 사용된다.

<img width="121" alt="스크린샷 2022-01-21 오후 3 12 02" src="https://user-images.githubusercontent.com/98019312/150487375-cfbcd338-da14-48fa-a437-01bfd4eaaef2.png">
<img width="403" alt="스크린샷 2022-01-21 오후 3 13 24" src="https://user-images.githubusercontent.com/98019312/150487380-f5ef7ef8-6738-461c-83d3-b047ebb52671.png">


0 ~ 1 사이의 범위를 가져 확률로서의 의미를 가질 수 있다. 

**하이퍼볼릭 탄젠트(tanh, Hyperbolic tangent) 함수**

<img width="394" alt="스크린샷 2022-01-21 오후 11 32 36" src="https://user-images.githubusercontent.com/98019312/150564491-53a10073-554c-4a05-b3de-856d4ea7e084.png">

하이퍼볼릭 탄젠트는 시그모이드 함수와 달리 0을 기준으로 대칭점을 확인할 수 있어 학습 수렴 속도가 빠르다고 할 수 있다.

**ReLU(Rectified Linear Unit) 함수**

시그모이드 함수는 출력하는 값의 범위가 0 ~ 1 사이 이므로 레이어를 거치면 거칠 수록 값이 현저하게 작아지게 되어 기울기 소실(Vanishing gradient)현상이 발생한다. 이를 해결하기 위해 렐루 함수가 등장하였다. 


<img width="153" alt="스크린샷 2022-01-21 오후 11 13 47" src="https://user-images.githubusercontent.com/98019312/150564541-335cb563-7448-46aa-8404-01a72746f4d2.png">
<img width="476" alt="스크린샷 2022-01-21 오후 11 20 06" src="https://user-images.githubusercontent.com/98019312/150564550-9db89487-e06a-4360-9b02-787df4047c67.png">


렐루 함수는 간단하다. + 신호는 그대로, -신호는 차단하는 함수

렐루 함수는 은닉층에서 많이 사용된다. 왜냐하면 기울기 소실이 발생하지 않기 때문이다. (??)

출력 값의 범위가 넓고 양수인 경우 자신을 그대로 반환하기 때문에 시그모이드에서 발생한 기울기 손실 문제점을 일으키지 않는다. 

Q. 그러면 쓰지 않으면 되지 않는가 ?

애초에 활성화 함수는 왜 쓰는가 ??

딥러닝 모델의 표현력을 향상시켜준다고 한다. 선형 결과를 가져오는 함수를 활성화 함수 없이 계속된 연산을 수행하는 결과 선형 결과를 내보인다. 하지만 복잡한 특징을 가진(비선형 데이터) 파라미터를 추출하기 위해서는 비선형을 지니고 있어야 한다.

하이퍼볼릭 탄젠트, 시그모이드보다 6배 더 빠르다고 한다.

**소프트맥스 함수 (softmax)**

세개 이상으로 분류하는 다중 클래스 분류에서 사용되는 활성화함수다.

분류될 클래스가 n개라고 할때 n차원의 벡터를 입력받아 각 클래스에 속할 확률을 추정한다.

만약 분류될 클래스가 3개라고 한다면 다음과 같이 계산된다.  (e 상수)
<img width="459" alt="스크린샷 2022-01-21 오후 3 22 42" src="https://user-images.githubusercontent.com/98019312/150487318-8a4866ea-5a1c-4815-884e-0f1e489d767c.png">
<img width="492" alt="스크린샷 2022-01-21 오후 3 25 12" src="https://user-images.githubusercontent.com/98019312/150487393-aaa0c59b-ed60-463c-a100-3009fa2de4eb.png">


**확률적 경사 하강법(Stochastic Gradient Descent)**

- 손실함수를 최소화 하기 위해선 1) optimizer(최적화) 2) 경사 하강법 방법을 사용한다.
<img width="373" alt="스크린샷 2022-01-21 오후 3 32 34" src="https://user-images.githubusercontent.com/98019312/150487480-fb82a3bc-6050-4000-9c1a-cb9f1453e0fc.png">


하지만 경사 하강법에는  한계가 존재한다. 

경사 하강법은 신경망에서 출력되는(딥러닝) 예측값과 실제 값 차이인 손실함수(loss funciton)의 값을 최소화 하는 것이 목적이다. 그러나 학습 데이터 셋이 많아질 수록 계산량은 많아지고 이로 인한 학습  속도가 느려지게 된다.
<img width="461" alt="스크린샷 2022-01-21 오후 3 36 48" src="https://user-images.githubusercontent.com/98019312/150487523-45193b00-76aa-4c1f-9bd4-c2dea43286ce.png">

또한 실제 위처럼 매끈한 2차 곡선 형태가 나오지 않는다. 이로 인해 이동거리만큼 이동 하더라고 최적  해에 수립하지 못할 수도 있다. (local minimum 현상)

또한 plateau라는 곳에서의 학습 속도가 매우 느리다. 이동 거리 = 현 지점의 기울기 X 학습률인데 기울기가 0에 가까워 질 수록 이동거리가 줄어 들어 더이상 학습이 일어나지 않는 가중치 소멸(gradient vanishing) 현상이 발생할 수 도 있다. 

또한 가중치(w)가 여러개 있을 경우 각각의 가중치마다 스케일이 다르면 다음과 같은 현상이 나타난다.

<img width="542" alt="스크린샷 2022-01-21 오후 3 39 54" src="https://user-images.githubusercontent.com/98019312/150487445-18c213dd-16fb-4dac-8a49-7092655aa26f.png">


w1의 스케일이 w2보다 크다보니 손실함수는 w1의 변화에 매우 둔감하고 w2에 매우 민감하다. 따라서 w2가 조금만 변해도 손실함수는 크게 변화게 되어 두 매개변수에 따른 손실 함수 변화가 일정하지 않다. 실제 매개 변수를 수백만개에 달할 정도로 많을테니 이러한 지그재그식 모습은 더욱 복잡해지고 이로 인해 최적의 해를 보장받기 쉽지 않다.

참고 : [https://gooopy.tistory.com/67?category=824281](https://gooopy.tistory.com/67?category=824281)

이러한 sgd 방식의 순간순간의 기울기에 따라 방향을 결정하고 탐색하는 단점을 해결하고자 momentum 방식을 사용한다. 

**momentum(모멘텀)**

<img width="169" alt="스크린샷 2022-01-23 오후 5 43 01" src="https://user-images.githubusercontent.com/98019312/150672091-3693e957-94b0-4990-b5dd-a9918d8c5822.png">

모멘텀의 속도 갱신 수식

<img width="118" alt="스크린샷 2022-01-23 오후 5 43 49" src="https://user-images.githubusercontent.com/98019312/150672080-693bc96a-691d-4c56-b5a6-c96953777095.png">


모멘텀 가중치 갱신 수식

***W*** 는 갱신할 가중치 매개변수,

***L***은 손실함수를 나타내고

***η*** 는 학습률 learning rate,

***∂L/∂W***은 ***W***에 대한 손실함수의 기울기를 나타냅니다.

기본 값은 0이나 0보다 큰 값으로 지정하면 이전 그레디언트를  가속도처럼 사용하게 된다.

**nesterov momentum(네스테로프 모멘텀)**

모멘텀 최적화를 2번 반복하여 수행 

모델 학습의 단위 - epoch, batch, size, iteration 

**AdaGrad 최적화 기법**

각 매개변수에 적응적으로 학습률을 조정하며 학습을 진행한다.

(아직 수식 잘 모르겠음..)

> *AdaGrad는 과거의 기울기를 제곱하여 계속 더해가기 때문에 학습을 진행할수록 갱신 강도가 약해집니다. 무한히 계속 학습할 경우에는 어느 순간 갱신량이 0이 되어 전혀 갱신되지 않게 됩니다. 이 문제를 개선한 기법으로 RMSProp이라는 방법이 있습니다. RMSProp는 과거의 모든 기울기를 균일하게 더하지 않고 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영합니다. 이를 지수이동평균 Exponential Moving Average, EMA라고 하고 과거 기울기의 반영 규모를 기하급수적으로 감소시킵니다.*
>

**모델 학습의 단위 - epoch, batch, size, iteration**

**epoch**

훈련 데이터 셋들이 모델을 통과한 횟수

1epoch는 훈련 데이터셋 전체가 한 신경망에 적용되어 순전파와 역전파를 통해 신경망을 한번 통과했다는 의미.

epoch를 높일 수록 적합한 파라미터 찾을 확률이 올라간다. ( overfitting 주의)

**batch size**

연산 한번에 들어가는 데이터의 크기, 그 데이터 크기만큼의 데이터셋 → mini batch

1회 epoch에 m개 이상의 mini batch가 들어가게 되며 m = 1인 경우 배치 학습법이라고 한다.

bacth size가 너무 큰 경우 한번에 처리해야 할 데이터의 양이 많아지므로 학습 속도가 느려지게 되고 메모리 부족 문제가 나타날 수 있다.

batch size가 너무 작은 경우 적은 양의 데이터로 가중치를 업데이트하고 업데이트가 자주 발생하므로 좋지 x

**iteration**

각 배치마다 파라미터 업데이트가 한번씩 일어나므로 iteration은 파라미터 업데이트 횟수와 같다.

1. 배치 경사 하강법(Batch Gradient Desent, BGD)
    
    손실함수 기울기 계신에서 전체 학습 데이터를 대상으로 동일하게 잡는 방법
    
2. 확률적 경사 하강법(Stochastic Gradient Desent, SGD)
    
    전체 훈련 데이터 셋으로 학습하는 것은 매우 비효율적 → 학습 데이터 셋에서 무작위로 한개의 샘플 데이터를 꺼내 그 데이터에 대해서만 기울기를 계산하는 것
    
    학습 속도가 매우 빠르다. ↔ 안정적이지 못하다. → 미니 배치 방법이 등장
    
3. 미니 배치 하강법(mini batch Gradient Desent)
    
    여러개의 데이터를 꺼내 수행.
    
    배치의 크기는 총학습 데이터 셋의 크기를 배치 크기로 나누었을때 떨어지는 크기로 하는 것이 좋다. 
    

**교차 검증(cross validation)**

고정된 test set을 통해 모델의 성능을 검증하고 수정하는 과정을 반복하면 내가 만든 모델은 test set에만 잘 동작된다는 문제점을 가진다. 이를 해결하고자 하는 것이 바로 교차 검증이다.

1. K-Fold Cross Validation (5 or 10을 많이 사용) → 회귀 모델일 경우
2. Stratifield K-Fold Cross Validation → 분류 모델일 경우

**Grid-Search**

모델에 가장 적합한 하이퍼파라미터 찾기 sklearn의 GridSearchCV 이용

```python
import pandas as pd

wine = pd.read_csv("https://bit.ly/wine_csv_data")

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

#sub 추출을 위해 2번 적용
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size = 0.2, random_state = 42)
sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size = 0.2, random_state = 42)

#grid-search - for best hyperparameter
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
params = {'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}
#n_jobs 병렬 실행에 사용할 CPU 수 -1 -> 시스템 내 모든 코어 사용
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs= -1)
gs.fit(train_input, train_target)
dt = gs.best_estimator_
print(dt.score(train_input, train_target))

#0.9615162593804117
```

### **순전파와 역전파**

신경망 모델에서 입려층으로부터 출력층까지 데이터가  순방향으로 전파되는 과정을 순전파라고 한다. 반대 방향으로 전파되는 것을 역전파라고 하는데 역전파는 순전파에서 발생한 오차를 줄이기 위해 새로운 가중치를 업데이트하고 새로운 가중치로 다시 학습하는 과정을 말한다. 역전파의 오차가 0에 가까워질때까지 반복한다. 역전파 알고리즘 실행시 가중치를 결정하는 방법에서는 경사 하강법이 사용된다.


