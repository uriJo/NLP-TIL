Bidirectional Encoder Representations from Transformers

2018년에 구글이 공개한 훈련된 모델

1. **개요**
    
    bert는 word2vec과 같은 문맥이 없는 임베딩 모델과 다르게 문맥을 고려한 모델이다.
    
    A 문장 : he got bit by python
    
    B 문장 : python is my favorite programming language
    
    word2vec이 문맥 독립 모델이기 때문에 두 문장에서 동일한 단어가 쓰였으므로 동일한 임베딩을 제공한다. 
    
2. **bert의 동작 방식**
    
    이름에서 알 수 있듯 transformer 모델을 기반으로 하여 인코더만을 사용한다.
    
3. **BERT의 구조**
    1. BERT-base
        
        12개의 인코더 레이어가 스택처럼 쌓인 형태.
        
        12개의 어텐션 헤드를 사용하며 피드포워드 네트워크는 768개 차원의 은닉 유닛으로 구성된다,
        
    2. BERT-large
        
        24개의 인코더 레이어가 스택처럼 쌓인 형태.
        
        16개의 어텐션 헤드 사용하며 피드포워드 네트워크는 1024개의 은닉 크기로 구성
        
4. **BERT의 사전 학습**
    
    모델을 학습시킬때 방대한 데이터 셋으로 학습 시키고 학습된 모델을 저장한다. 그 다음으로 새 태스크가 주어지면 임의 가중치로 모델을 초기화하는대신 이미 학습된 모델의 가중치로 모델을 초기화한다.
    
    즉 모델이 이미 대규모 데이터셋에서 학습 되었으므로 새 태스크를 위해 새로운 모델로 처음부터 학습시키는 대신 사전 학습된 모델을 사용하고 새로운 태스크에 따라 가중치를 조정한다. (파인튜닝)
    
    BERT는 MLM과 NSP라는 두가지 태스크를 이용해 거대한 코퍼스를 기반으로 사전 학습된다. 사전 학습 후 사전 학습된 BERT 모델을 저장해두고 새 태스크가 주어질 경우 BERT를 처음부터 학습시키는 대신 사전 학습된 BERT를 사용한다. 
    
    BERT에 데이터를 입력하기 전에 세가지 임베딩 레이어를 기반으로 입력 데이터를 임베딩으로 변환해야한다.
    
    - 토큰 임베딩
    - 세그먼트 임베딩
    - 위치 임베딩
    

사전학습과 파인 튜닝의 차이를 줄이기 위해 80-10-10 기법을 사용한다. 

BERT에선 갤루(GELU(gaussian error linear unit)라는 활성화 함수를 사용한다.


<img width="479" alt="스크린샷 2022-01-27 오전 11 58 20" src="https://user-images.githubusercontent.com/98019312/151305731-268611ef-3ebd-4808-863c-87c39dfe3784.png">


**어휘 사전 생성시 사용되는 하위 단어 토큰화 알고리즘**

- 바이트 쌍 인코딩(byte pair encoding)
- 바이트 수준 바이트 쌍 인코딩(byte-level byte pair encoding)
- 워드피스(wordPiece)

1. **바이트 페어 인코딩(byte pair encoding)(BPE)**
    
    **OOV(Out-Of-Vocabulary) 문제를 해결하기 위해 모든 단어를 문자로 나누고 빈도수를 함께 저장한다.**
    
    ```python
    low : 5, lower : 2, newest : 6, widest : 3
    ```
    
    이 딕셔너리로부터 어취 사전을 생성한다. how ? 
    
    어휘 사전의 크기를 N이라 할때 고유 문자 + N-1개의 빈번도 있는 쌍들을 어휘사전으로 만든다.
    
    N이 다 찰때까지 반복한 뒤 종료한다.
    

    그리고 OOV 단어가 등장할때 그 단어가 어휘사전에 있다면 바로 출력하고(토큰화 x), 없다면 하위 단어로 나눠 다시 어휘 사전에 있는지 검색한다. (실제 어휘 사전이 커서 문자 단위로 모든 단어들이 토큰화 안되는 경우는 없다)

1. **바이트 수준 바이트 페어 인코딩(BBPE)**
    
    단어를 바이트 페어 인코딩에서와 같이 문자로 변환하는 것이 아닌 바이트 시퀀스로 변환한다. 다국어 설정에 매우 유용한다.
    

1. **워드피스**
    
    **바이트 페어 인코딩에선** N-1개의 빈번도 있는 쌍을 어휘 사전에 잠재적 후보로 두었다. 워드피스에선 기호쌍 s 및 t의 가능도를 P(st)/ P(s) P(t) 로 계산한다. 가능도가 높으면 기호 쌍을 병합하고 어휘 사전에 추가한다.
    

**BERT 활용하기**

tokens = [I, love, Paris]  → tokens = [[CLS], I, love, Paris] ⇒CLS 토큰 추가

토큰 길이를 7로 유지한다 할때

tokens = [[CLS], I, love, Paris, [SEP], [PAD], [PAD]] 로 만듬

attension_mask  [ 1,1,1,1,1, 0,0] ⇒ PAD 토큰은 0 tjfwjd

token_ids = [101, 1045, 2293, 3000, 102, 0, 0] ⇒  모든 토큰을 고유한 토큰 ID에 매핑 

**3.4.1 사전 학습된 BERT 모델 파인 튜닝하기**

사전  학습 된 BERT 모델을 특징 추출기로 사용하면 사전 학습된 BERT 모델이 아닌 분류기의 가중치만  업데이트 하게 된다. 

- 텍스트 분류
- 자연어추론 (NLI (inference)) - 모델이 주어진 전제에 참인지 거짓인지 중립인지 여부를 결정하는 태스크
- 질문 응답 - 질문에 대한 응답이 포함된 단락과 함께 질문이 제공된다.
- 개체명 인식


**BERT와 GPT 비교**

GPT는 언어모델이다. 이전 단어가 주어졌을때 다음 단어가 무엇인지 맞히는 과정에서 프리트레인한다. 문장 왼쪽에서 오른쪽으로 단방향 계산적이다.

BERT는 마스크 언어모델이다. 문장 중간에 빈칸을 만들고 해당 빈칸에 어떤 단어가 적절한지 맞히는 과정에서 프리트레인한다. 빈칸 앞뒤 문맥을 모두 살핀다는 점에서 양방향 계산을 수행한다. 

이 때문에 GPT는 문장 생성에 BERT는 문장 의미를 추출하는데 강점을 지니는 것으로 알려져있다. 

**GPT의 구조**

트랜스포머에서 인코더를 제외하고 디코더만을 사용한다. 

**BERT의 파생 모델**

**ALBERT란**

BERT의 주요 문제점 중 하나는 수백만개 변수로 구성되어 있다는 점이다. BERT-base는 1억 1천만개의 변수로 구성되어 있으며 모델 학습이 어렵고 추론 시간이 많이 걸린다. 모델 크기를 늘리면 성능은 좋지만 계산시 리소스 제한이 발생한다. 이를 위해 ALBERT를 도입하였다.

BERT에 비해 다음과 같은 두가지 기법을 사용해 변수를 젝게 사용한다.

- **크로스 레이어 변수 공유**
    
    BERT는 N개의 인코더로 구성되어 있다. 예를 들어 BERT-base는 12개의 레이어로 구성되어 있다. 학습이 진행되면 인코더 레이어에 있는 모든 변수들에 대해 학습이 이루어진다. 하지만 크로스 레이어 변수 공유 방법은 모든 인코더 레이어의 변수를 학습시키는 것이 아니라 첫번째 인코더 레이어의 변수만 학습한 다음 첫번째 인코더 레이어의 변수를 다른 모든 인코더 레이어와 공유한다. 
    
    
    <img width="458" alt="스크린샷 2022-01-28 오후 10 47 03" src="https://user-images.githubusercontent.com/98019312/151561467-1876a57a-7d3d-4026-8898-77e91e065814.png">)
    
    
    BERT의 모델이 다음과 같을때 크로스 레이어 변수 공유 방식은
    
    all-shared : 첫번째 인코더의 레이어에 있는 모든 변수들을 나머지 인코더들과 공유, 
    
    shared feedforward network : 첫번째 인코더의 레이어의 피드포워드 네트워크 변수만 다른 레이어의 피드포워드 네트워크와 공유,
    
    shared attention : 첫번째 인코더 레이어의 멀티 헤드 어텐션의 변수만 다른 인코더 레이어와의 공유 
    
    이 3가지로 나뉜다. 
    
- **팩토라이즈 임베딩 레이어 변수화**
    
    워드피스 임베딩 크기는 은닉 레이어의 임베딩 크기와 같다. 따라서 은닉 레이어의 크기를 크게 설정하면 학습해야할 변수가 늘어난다. 이러한 현상을 방지하기 위해 임베딩을 더 작은 행렬로 분해하는 방법인 팩토라이즈 임베딩 변수화 방법을 사용한다.
    
    V : 사전의 크기, H : 임베딩 크기, E : 워드피스 임베딩 크기
    
    행렬 분해를 사용해 V x H를 VxE, ExH로 만든다.
    
    예를 들면 V : 30000이고 E를 은닉 레이어 크기와 같게 설정하지 않고 128로 두고 은닉 레이어 크기를 768로 한다면
    
    1. 원-핫인코딩한 벡터 V를 워드피스 임베딩 공간 E로 투영(VxE)
    2. 워드피스 임베딩 공간 E를 은닉 레이어 H로 투영(ExH)
    

**ALBERT의 모델 학습**

BERT의 경우 MLM과 NSP 태스크를 통해 사전 학습을 하지만 ALBERT는 MLM과 SOP(문장 순서 예측)을 사용한다.

[https://arxiv.org/pdf/1909.11942.pdf](https://arxiv.org/pdf/1909.11942.pdf)

**RoBERTa**

BERT와 기본적으로 동일하며 사전 학습시

- MLM 태스크에서 정적 마스킹이 아닌 동적 마스킹 방법을 사용했다.
    
    MLM 태스크의 경우 주어진 토큰 15% 확률로 무작위로 마스크된 토큰으로 변경한 후 모델에서 해당 토큰을 예측한다. 이때 마스킹은 데이터 전처리 단계에서 한번만 수행되며 에폭 별로 동일한 마스킹을 예측하도록 모델 학습을 한다. ← 정적 마스킹 그에 반해 한 문장을 10번 복사해 무작위 문장에 에폭별로 다른 마스킹된 문장을 학습시킨다.  예를 들어 문장 1의 경우 에폭1, 에폭 11, 에폭 21, (에폭은 40까지) // 문장 2의경우 에폭2, 에폭 12, .. 학습시킨다.
    
- NSP 태스크를 제거하고 MLM 태스크만 학습에 사용했다.
    
    NSP가 BERT 모델 사전 학습에 유용하지 않다는 사실을 발견해 MLM만 사용한다. 
    
- 배치 크기를 증가시켰다.
- 토크나이저로 BBPE를 사용했다.
    
    BERT의 경우 워드피스 토크나이저를 사용했다. ↔ BBPE를 사용한다.
