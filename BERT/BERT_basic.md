Bidirectional Encoder Representations from Transformers

2018년에 구글이 공개한 훈련된 모델

1. **개요**
    
    bert는 word2vec과 같은 문맥이 없는 임베딩 모델과 다르게 문맥을 고려한 모델이다.
    
    A 문장 : he got bit by python
    
    B 문장 : python is my favorite programming language
    
    word2vec이 문맥 독립 모델이기 때문에 두 문장에서 동일한 단어가 쓰였으므로 동일한 임베딩을 제공한다. 
    
2. **bert의 동작 방식**
    
    이름에서 알 수 있듯 transformer 모델을 기반으로 하여 인코더만을 사용한다.
    
3. **BERT의 구조**
    1. BERT-base
        
        12개의 인코더 레이어가 스택처럼 쌓인 형태.
        
        12개의 어텐션 헤드를 사용하며 피드포워드 네트워크는 768개 차원의 은닉 유닛으로 구성된다,
        
    2. BERT-large
        
        24개의 인코더 레이어가 스택처럼 쌓인 형태.
        
        16개의 어텐션 헤드 사용하며 피드포워드 네트워크는 1024개의 은닉 크기로 구성
        
4. **BERT의 사전 학습**
    
    모델을 학습시킬때 방대한 데이터 셋으로 학습 시키고 학습된 모델을 저장한다. 그 다음으로 새 태스크가 주어지면 임의 가중치로 모델을 초기화하는대신 이미 학습된 모델의 가중치로 모델을 초기화한다.
    
    즉 모델이 이미 대규모 데이터셋에서 학습 되었으므로 새 태스크를 위해 새로운 모델로 처음부터 학습시키는 대신 사전 학습된 모델을 사용하고 새로운 태스크에 따라 가중치를 조정한다. (파인튜닝)
    
    BERT는 MLM과 NSP라는 두가지 태스크를 이용해 거대한 코퍼스를 기반으로 사전 학습된다. 사전 학습 후 사전 학습된 BERT 모델을 저장해두고 새 태스크가 주어질 경우 BERT를 처음부터 학습시키는 대신 사전 학습된 BERT를 사용한다. 
    
    BERT에 데이터를 입력하기 전에 세가지 임베딩 레이어를 기반으로 입력 데이터를 임베딩으로 변환해야한다.
    
    - 토큰 임베딩
    - 세그먼트 임베딩
    - 위치 임베딩
    

사전학습과 파인 튜닝의 차이를 줄이기 위해 80-10-10 기법을 사용한다. 

BERT에선 갤루(GELU(gaussian error linear unit)라는 활성화 함수를 사용한다.


<img width="479" alt="스크린샷 2022-01-27 오전 11 58 20" src="https://user-images.githubusercontent.com/98019312/151305731-268611ef-3ebd-4808-863c-87c39dfe3784.png">


**어휘 사전 생성시 사용되는 하위 단어 토큰화 알고리즘**

- 바이트 쌍 인코딩(byte pair encoding)
- 바이트 수준 바이트 쌍 인코딩(byte-level byte pair encoding)
- 워드피스(wordPiece)

1. **바이트 페어 인코딩(byte pair encoding)(BPE)**
    
    **OOV(Out-Of-Vocabulary) 문제를 해결하기 위해 모든 단어를 문자로 나누고 빈도수를 함께 저장한다.**
    
    ```python
    low : 5, lower : 2, newest : 6, widest : 3
    ```
    
    이 딕셔너리로부터 어취 사전을 생성한다. how ? 
    
    어휘 사전의 크기를 N이라 할때 고유 문자 + N-1개의 빈번도 있는 쌍들을 어휘사전으로 만든다.
    
    N이 다 찰때까지 반복한 뒤 종료한다.
    

    그리고 OOV 단어가 등장할때 그 단어가 어휘사전에 있다면 바로 출력하고(토큰화 x), 없다면 하위 단어로 나눠 다시 어휘 사전에 있는지 검색한다. (실제 어휘 사전이 커서 문자 단위로 모든 단어들이 토큰화 안되는 경우는 없다)

1. **바이트 수준 바이트 페어 인코딩(BBPE)**
    
    단어를 바이트 페어 인코딩에서와 같이 문자로 변환하는 것이 아닌 바이트 시퀀스로 변환한다. 다국어 설정에 매우 유용한다.
    

1. **워드피스**
    
    **바이트 페어 인코딩에선** N-1개의 빈번도 있는 쌍을 어휘 사전에 잠재적 후보로 두었다. 워드피스에선 기호쌍 s 및 t의 가능도를 P(st)/ P(s) P(t) 로 계산한다. 가능도가 높으면 기호 쌍을 병합하고 어휘 사전에 추가한다.
    

**BERT 활용하기**

tokens = [I, love, Paris]  → tokens = [[CLS], I, love, Paris] ⇒CLS 토큰 추가

토큰 길이를 7로 유지한다 할때

tokens = [[CLS], I, love, Paris, [SEP], [PAD], [PAD]] 로 만듬

attension_mask  [ 1,1,1,1,1, 0,0] ⇒ PAD 토큰은 0 tjfwjd

token_ids = [101, 1045, 2293, 3000, 102, 0, 0] ⇒  모든 토큰을 고유한 토큰 ID에 매핑 

**3.4.1 사전 학습된 BERT 모델 파인 튜닝하기**

사전  학습 된 BERT 모델을 특징 추출기로 사용하면 사전 학습된 BERT 모델이 아닌 분류기의 가중치만  업데이트 하게 된다. 

- 텍스트 분류
- 자연어추론 (NLI (inference)) - 모델이 주어진 전제에 참인지 거짓인지 중립인지 여부를 결정하는 태스크
- 질문 응답 - 질문에 대한 응답이 포함된 단락과 함께 질문이 제공된다.
- 개체명 인식
