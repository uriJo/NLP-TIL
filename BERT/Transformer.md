## Transformer

트랜스포머가 출현한 뒤로 다양한 태스크에서 활용되었던 순환 신경망(RNN), 장단기 메모리(LSTM)은 트랜스 포머로 대체된다. 

RNN과 LSTM 네트워크는 다음 단어 예측, 기계 번역, 텍스트 생성 등 순차적 태스크에서 널리 사용된다. 

하지만 장기 의존성 문제를 가진다. 이런 RNN의 한계를 극복하려고 

Attention Is All You Need 논문에선 트랜스포머라는 아키텍쳐를 제안한다.

트랜스포머는 RNN에서 사용한 순환 방식을 사용하지 않고 순수하게 어텐션만 사용한 모델이다.

어텐션과 같이 인코더와 디코더로 구성된다.

먼저 인코더에 입력 문장을 입력하면 인코더는 입력 문장의 표현 방법을 학습시키고 그 결과를 디코더로 보낸다. 디코더는 인코더에서 학습한 표현 결과를 입력 받아 사용자가 원하는 문장을 생성한다.

### 1.2 인코더

인코더의 내부를 보면 

- 멀티 헤드 어텐션
- 피드포워드 네트워크

두가지로 구성되어 있다.

**1.2.1 셀프 어텐션**

예를 들어 input sentence → I am good 이 있을때

워드 임베딩을 통해

[1.76, 2.22, ..., 6.66]

[7.77, 2.35, ..., 6.53]

[11.32, 24.21, ..., 9.24]

3x512로 되어 있을때

입력 행렬 x로부터 쿼리(Q), 키(K), 값(V) 행렬을 생성하는데 각각의 가중치 행렬 Wq, Wk, Wv 를 만들고 입력 행렬 X에 곱해 만든다. 

1단계

쿼리(Q)와 키(K)행렬의 전치 행렬을 내적 연산 한다. → 두 벡터가 얼마나 유사한지 알 수 있다.

즉  쿼리 벡터(q1)과 키 벡터(k1, k2, k3) 사이 내적 연산은 쿼리 벡터 q1(I)와 키 벡터 k1(I), k2(am), k3(ggod) 사이 유사도를 구한 것이다. 이와 같이 쿼리 벡터(q2), (q3)에 대해서도 키벡터와 내적 연산을 통해 유사도를 구한다.

2단계

1단계에서 구한 QKt 행렬의 키 벡터 차원의 제곱근 값으로 나눈다. 

3단계

소프트맥스 함수를 통해 정규화 작업을 진행한다.

벡터들의 값이 0~1사이 값들을 가진다. 이러한 값들을 스코어 행렬이라고 한다. 위 점수들을 바탕으로 문장 내에 있는 각 단어가 문장에 있는 전체 단어와 얼마나 연관 되어 있는지 알 수 있다. 

4단계

어텐션 행렬을 계산한다.

앞에서 계산한 스코어 행렬에 값 행렬(V)를 곱해 어텐션 (Z) 행렬을 구한다.

**1.2.2 멀티 헤드 어텐션 원리**

어텐션을 사용할때 한개만 사용한 형태가 아닌 헤드 여러개를 사용한 어텐션 구조도 사용 할 수 있다. 

**1.2.3 위치 인코딩으로 위치 정보 학습**

트랜스포머 네트워크에서는 단어 단위로 문장을 입력하는 대신 문장 안에 있는 모든 단어를 병렬 형태로 입력한다. 병렬로 단어를 입력하는 것은 학습 시간을 줄이고 RNN의 장기 의존성 문제를 해결하는데 도움이 된다.

하지만 단어의 순서 정보를 유지 하지 않고 문장 의미를 어떻게 이해 할 수 있는지 문제가 있다. 

입력 행렬을 트랜스포머에 바로 입력하면 단어의 순거 정보를 이해 할 수 없다. 입력 행렬을 트랜스포머에 직접 전달하는대신 네트워크에서 문장의 의미를 이해할 수 있도록 단어의 순서를 표현하는 정보를 추가로 제공해야 한다. 이를 위해 위치 인코딩이라는 새로운 방법을 활용한다. 이름에서 알 수 있듯 문장에서 단어의 위치를 나타내는 인코딩이다. 

<img width="256" alt="스크린샷 2022-01-25 오후 3 11 50" src="https://user-images.githubusercontent.com/98019312/151002601-d9ab4eb3-62bd-4aa8-a10a-0eaa8c12fe9a.png">


**1.2.4 피드포워드 네트워크**

2개의 전결합층과 ReLU 활성화 함수로 구성된다. 

### **1.3 디코더**

디코더는 시간 스탭 t = 1이라면 입력값은 문장의 시작을 알리는 <sos>를 입력 한다. 이 입력 값을 받은 디코더는 타깃 문장의 첫번째 단어 je를 생성한다.

시간 스탭 t= 2라면 현재까지 입력에 이전 단계 t-1 디코에서 생성한 단어를 추가해 문장의 다음 단어를 생성한다. 쭉 진행하고 <eos> 토큰을 생성할때 완료된다. 

디코더 역시 위치 인코딩을 추가한 값을 디코더의 입력 값으로 사용한다.

디코더는 

- 피드포워드
- 멀티 헤드 어텐션
- 마스크된 멀티 헤드 어텐션

3가지로 구성되어 있다.

**1.3.1 마스크된 멀티 헤드 어텐션**

모델을 학습할때는 이미 타깃 문장을 알고 있어 디코더에 기본으로 차깃 문장 전체를 입력하면 되지만 수정 작업이 필요하다. 디코더에서 문장을 입력할때 처음에는 <sos> 토큰을 입력하고 <eos> 토큰이 생성될 때까지 이전 단계에서 예측한 단어를 추가하는 형태로 입력을 반복한다.

디코더의 입력 행렬을 x라고 하자

행렬 x를 디코더에 입력하면 첫번째 레이어는 마스크된 멀티 헤드 어텐션이 된다. 인코더에서 사용한 멀티 헤드 어텐션과 다른점이 한가지 있다. 셀프 어텐션을 구현하면 처음에는 쿼리(Q), 키(K), 값(V) 행렬을 생성한다. 멀티 어텐션을 계산하면 h개의 쿼리(Q), 키(K), 값(V) 행렬을 생성한다. 각 단어의 의미를 이해하기 위해 각 단어와 문장 내 전체 단어를 연결했다. 

그런데 디코더에서 문장을 생성 할때 이전 단계에서 생성한 단어만 입력 문장으로 넣는다는 점이 중요하다. 

### **전이 학습(Transfer Learning)**

특정 태스크(태스크1)를 학습한 모델을 다른 태스크(태스크2) 수행에 재사용 하는 기법

여기서 태스크 1을 업스트림(upstream) 태스크라 부르고 태스크2는 다운스트림(downstream)이라고 부른다. 태스크 1은 다음 단어 맞히기, 빈칸 채우기 등 대규모 말뭉치의 문맥을 이해하는 과제이며 태스크2는 문서분류, 개체명 인식 등 우리가 풀고자 하는 자연어 처리의 구체적인 문제들이다.

업스트림 태스크를 학습하는 과정을 **프리트레인**이라고 한다. 

업스트림 태스크 중  빈칸 패우기로 업스트림 태스크를 수행한 모델은 마스크 언어 모델이라고 한다. 학습 데이터만 있으면 데이터 안에서 정답을 만들고 이를 바탕으로 모델을 학습하는 방법을 자기 지도 학습(self supervised learing)이라고 한다. 

우리가 업스트림을 프리트레인한 이유는 다운 스트림을 잘 하기 위해서다.

다운 스트림의 본질은 분류이다. 

**파인튜닝**이란 프리트레인을 마친 모델을 다운 스트림 태스크에 맞게 업데이트 하는 기법을 말한다. 예를 들어 문서 분류를 수행할 경우 프리트레인을 마친 BERT 모델 전체를 문서 분류 데이터로 업데이트 한다. 마찬가지로 개체명 인식을 수행한다면 BERT 모델 전체를 해당 데이터로 업데이트 한다. 

이 외에도 프롬프트 튜닝, 인컨텍스트튜닝, 제로샷튜닝, 원샷러닝, 퓨샷러닝 등 다양한다.
