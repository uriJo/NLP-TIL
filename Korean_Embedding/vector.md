***2장 vector.md***

임베딩을 만드는 방법

<img width="815" alt="스크린샷 2022-01-25 오후 11 50 18" src="https://user-images.githubusercontent.com/98019312/150999643-895a63c3-f5dc-4768-85ea-4bd4b02d202b.png">

**1) 백오브워즈 가정**

bag - 중복 원소를 허용한 집합 즉 원소의 순서는 고려하지 않는다.

문서 내의 단어 사용에 저자의 주제가 녹아 있다고 가정한다.

간단한 아이디어이지만 정보 검색 분야에 많이 쓰이고 있다. 사용자 질의에 가장 적절한 문서를 보여줄때 질의를 백오브 워즈 임베딩으로 변환하고 질이와 검색 대상 문서 임베딩 간 코사인 유사도를 구해 가장 높은 문서를 사용자에게 노출한다.

- TF-IDF
    
    단어의 수만 가지고는 한계를 지닌다. 왜 ? 조사(을/를, 이/가)와 같은 단어로 해당 문서의 주제를 추측하기 어렵다. 이를 보완히기 위해 제안된 기법이 Term Frequency-Inverse Document Frequency이다.
    
    <img width="678" alt="스크린샷 2022-01-25 오후 11 51 47" src="https://user-images.githubusercontent.com/98019312/150999799-4a441f6f-e1fe-4f98-80d0-7a2ffe86bb9f.png">

    #
    - log 를 붙였을 때
    <img width="195" alt="스크린샷 2022-01-25 오후 11 52 30" src="https://user-images.githubusercontent.com/98019312/150999929-4058d123-3260-4265-9bfc-0aa8390104c9.png">

    #
    - log 를 붙이지 않았을 때
    <img width="181" alt="스크린샷 2022-01-25 오후 11 53 04" src="https://user-images.githubusercontent.com/98019312/151000025-c3177f06-cb8a-426a-9427-ef055b969153.png">

    
    
    tf - y문서 내의 x 단어의 개수
    
    df -x 단어를 포함한 문서의 수
    
    불용어와 같은 단어들은 비교적 자주 쓰이지 않는 단어들보다 최소 수십 배 자주 등장한다. 따라서 idf값을 사용하는데 이는 df를 역수 취해 등장하는 총 문서수만큼 비례하여 나타낸 값을 의미한다. 이때 log를 씌우는 이유는 희소 단어들에 대해  가중치부여가 많이 되기 때문에 이를 완화시키기 위해서다
    
    즉 TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단합니다. TF-IDF 값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것입니다. 즉, the나 a와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF의 값은 다른 단어의 TF-IDF에 비해서 낮아지게 됩니다.
    
    - Deep-Averaging Network


**2) 언어 모델**
    
    단어 시퀀스에 확률을 부여하는 모델이다. 즉 가장 자연스러운 단어 시퀀스를 찾아내는 모델이다. ex > 기계 번역, 오타 교정, 음성 인식 ( 더 자연스러운 문장 찾기)
    
    
    단어 시퀀스의 확률
    <img width="445" alt="스크린샷 2022-01-25 오후 11 53 33" src="https://user-images.githubusercontent.com/98019312/151000156-4ddbc0ae-90f0-4cfe-a618-6141a241c4b2.png">
     #
    
    다음 단어가 등장할 확률(조건부 확률)
    <img width="251" alt="스크린샷 2022-01-25 오후 11 53 41" src="https://user-images.githubusercontent.com/98019312/151000212-bdb56449-d735-41db-8dfc-575a0c51fc11.png">

    
    
    - 통계 기반 언어 모델
        
        n-gram (n개의 단어)
        
        단어의 빈도를 세어 학습
        
    - 뉴럴 네트워크 기반 언어 모델
        
        주어진 단어 시퀀스 가지고 다음 단어 맞추는 과정에서 학습 ex> ELMo, GPT 등
        
    
**3) 분포 가정**
    
    분포란 특정 범위, 즉 윈도우 내에 동시에 등장하는 이웃 단어 또는 문맥의 집합을 의미한다. 
    
    개별 단어의 분포는 그 단어가 문장 내에서 주로 어느 위치에 나타나는지 이웃한 위치에 어떤 단어가 주자 나타나는지에 따라 달라진다. 어떤 단어쌍이 비슷한 문맥 환경에서 자주 등장한다면 그 믜미 또한 유사할 것이라는게 분포 가정의 전제이다 
    
    - PMI 점별 상호량
        
        두 확률 변수 사이 상관성.
        
        두 확률변수가 완전히 독립(independent)인 경우 그 값이 0이 된다. 독립이라고 함은 단어 A가 나타나는 것이 단어 B의 등장할 확률에 전혀 영향을 주지 않고, 단어 B 등장이 단어 A에 영향을 주지 않는 경우를 가리킨다. 반대로 단어 A가 등장할 때 단어 B와 자주 같이 나타난다면 PMI 값은 커진다. 즉, PMI는 두 단어의 등장이 독립일 때 대비해 얼마나 자주 같이 등장하는지를 수치화한 것이다. PMI 공식은 다음과 같다.
        <img width="357" alt="스크린샷 2022-01-25 오후 11 53 47" src="https://user-images.githubusercontent.com/98019312/151000242-c374b54f-6151-4aa7-9070-10f5281bb4a9.png">

        
        두 단어가 얼마나 자주 등장하는지에 관한 정보를 수치화한것으로 PMI 행렬의 행벡터를 해당 단어의 임베딩으로 사용 할 수도 있다.
        
        윈도우가 2라면 앞뒤 2개의 문맥 단어의 빈도를 계산한다. 
        
    - word2Vec
        
        CBOW 모델은 문맥 단어를 가지고 타깃 단어를 맞추는 것이고
        
        Skip-gram은 반대로 타킷 단어를 가지고 문맥 단어가 무엇일지 예측하는 과정에서 학습된다. 
        <img width="599" alt="스크린샷 2022-01-25 오후 11 53 53" src="https://user-images.githubusercontent.com/98019312/151000276-16a2fe68-bcd3-4ee7-a46f-717957c33a59.png">

        
