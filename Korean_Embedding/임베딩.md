## 1장 Embedding

**1.1 임베딩이란**

사람이 쓰는 자연어를 기계가 이해할 수 있는 벡터로 바꾸는 과정

**1.2 임베딩의 역할**

1) 단어/ 문장 간 관련도 계산

단어를 벡터로 표현하는 순간 단어간의 유사도를 계산 가능 ex> 코사인 유사도

2) 의미/문법 정보 함축

벡터간의 사친 연산으로 단어 간의 유추 평가 가능

ex> 단어1 + 단어 2 - 단어 3 = 아들 + 딸 - 소년 = 소녀

3) 전이 학습

단어를 임베딩 한 값이 input 으로 쓰인다. 

**1.3 임베딩 기법**

0) 그 전에 시소러스를 활용한 방법 **(시소러스 기반 기법)**

- wordNet
    
     nlp에서 가장 유명한 시소러스
    
    수많은 단어에 대한 동의어와 계층 구조 등 정의
    
    하지만 수작업으로 시대 변화에 대응하기 어렵다. 비용이 많이 든다. 단어간 미묘한 차이를 표현 할 수 없다.
    

1) 통계 기법에서 뉴럴 네트워크 기반으로 **(통계 기반 기법)**

    초기 잠재 의미 분석(단어의 사용 빈도, 말뭉치 통계량)에 특이값 분해와 같은 수학적 기법을 적용해 벡터의 차원을 축소하는 방법 사용

    하지만 말뭉치 데이터 셋에 들어간 단어가 많아 행이 커지고, 그에 반해 데이터 값은 대부분 0(희소 행렬)을 띈다. 따라서 원래 행의 차원을 축소하게 되는데 단어를 축소하는 것을 **단어 차원의 임베딩**, 문서를 기준으로 문서를 축소하는 것을 **문서 차원의 임베딩**이라고 한다.

    최근에는 뉴럴 네트워크 기반의 임베딩 기법들이 주목 받고 있다. 뉴럴 네트워크 기반의 모델들은 이전 단어가 주어졌을 때 다음 단어가 뭐가 될지 예측하거나 문장 내 빈칸에 들어갈 단어가 무엇인지에 관해 학습한다.  

2) 단어 수준에서 문장 수준으로 **(추론 기반 기법)**

    단어 수준 모델 ex> NLPM, Word2Vec, Glove, FastText, Swivel 

    단어 임베딩기법들은 각 벡터에 해당 단어의 문맥적 의미를 함축한다. 하지만 단어 수준의 임베딩 기법의 단점은 동음이의어 분간이 어렵고, 단어의 형태가 같다면 동일 단어로 보고 문맥 정보를 해당 단어 벡터에 투영하기 때문이다.

    2018년 이후 ELMo가 발표된 후 문장 수준 임베딩 기법들이 주목을 받게 된다. ex > GPT, BERT 등

    문장 수준의 임베딩 기법들은 개별 단어가 아닌 단어 시퀀스 전체의 문맥적 의미를 함축하기 때문에 단어 임베딩 기법보다 전이 학습 효과에 효과적이라고 한다. 

3) 롤 → 엔드투엔드 → 프리트레인/파인튜닝

    데이터 통째로 모델에 넣고 입출력 사이의 관계를 사람의 개입 없이 모델 스스로 처음부터 끝까지 이해하도록 하는 기법을 엔드투엔드라고 한다. ex>seq2Seq 

    2018년 이후 ELMo가 발표된 후 엔드투 엔드 방식에서 벗어나 프리트레인(pretrain), 파인튜닝(fine tuning) 방식으로 발전하고 있다. 우선 대규모 말뭉치로 임베딩을 만든다.(pretrain)

    이 임베딩에는 말뭉치의 의미적, 문법적 맥락이 포함되어 있다. 이후 임베딩을 입력으로 하는 새로운 딥러닝 모델을 만들고 우리가 풀고 싶은 구체적인 문제에 맞는 소규모 데이터에 맞게 임베딩을 포함한 모델 전체를 업데이트 한다. (fine tuning) 

    ex > ELMo, GPT, BERT

4) 임베딩 기법

    - 행렬 분해 기법 ex> Glove
    - 예측 기반 방법 ex> Word2Vec, fasttest, BERT, ELMo, GPT
    - 토빅 기반 방법 ex> LDA
